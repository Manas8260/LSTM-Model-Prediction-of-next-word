{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3625f2",
   "metadata": {},
   "source": [
    "## Project Description: Next Word Prediction Using LSTM\n",
    "#### Project Overview:\n",
    "\n",
    "This project aims to develop a deep learning model for predicting the next word in a given sequence of words. The model is built using Long Short-Term Memory (LSTM) networks, which are well-suited for sequence prediction tasks. The project includes the following steps:\n",
    "\n",
    "1- Data Collection: We use the text of Shakespeare's \"Hamlet\" as our dataset. This rich, complex text provides a good challenge for our model.\n",
    "\n",
    "2- Data Preprocessing: The text data is tokenized, converted into sequences, and padded to ensure uniform input lengths. The sequences are then split into training and testing sets.\n",
    "\n",
    "3- Model Building: An LSTM model is constructed with an embedding layer, two LSTM layers, and a dense output layer with a softmax activation function to predict the probability of the next word.\n",
    "\n",
    "4- Model Training: The model is trained using the prepared sequences, with early stopping implemented to prevent overfitting. Early stopping monitors the validation loss and stops training when the loss stops improving.\n",
    "\n",
    "5- Model Evaluation: The model is evaluated using a set of example sentences to test its ability to predict the next word accurately.\n",
    "\n",
    "6- Deployment: A Streamlit web application is developed to allow users to input a sequence of words and get the predicted next word in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10404662",
   "metadata": {},
   "source": [
    "## Data Collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1474fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd\n",
    "\n",
    "# Load Data\n",
    "data = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "with open('hamlet.txt','w') as file:\n",
    "    file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae0c4b",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf779b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user\\miniconda3\\envs\\venv1\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text  import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5187f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hamlet.txt\",'r') as file:\n",
    "    text = file.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c361054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating indexes for words\n",
    " \n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index)+1\n",
    "total_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4274f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d541e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input sequences\n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence  = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa02979c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 687],\n",
       " [1, 687, 4],\n",
       " [1, 687, 4, 45],\n",
       " [1, 687, 4, 45, 41],\n",
       " [1, 687, 4, 45, 41, 1886]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e452011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad sequences\n",
    "max_sequence_length = max([len(x) for x in input_sequences])\n",
    "max_sequence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "913b1fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    1,  687],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           1,  687,    4],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
       "         687,    4,   45],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    1,  687,\n",
       "           4,   45,   41],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    1,  687,    4,\n",
       "          45,   41, 1886]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre'))\n",
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd9621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create predictiors and label\n",
    "import tensorflow as tf \n",
    "x,y  = input_sequences[:,:-1],input_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a9298f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,    1],\n",
       "       [   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba88efe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 687,    4,   45, ..., 1047,    4,  193])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dee1842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.keras.utils.to_categorical(y,num_classes = total_words)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17719620",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = tts(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3117086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Our LSTM RNN model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8151edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "model =Sequential()\n",
    "model.add(Embedding(total_words,100,input_length = max_sequence_length-1))\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation = 'softmax'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cea2530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 13, 150)           150600    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13, 150)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219418 (4.65 MB)\n",
      "Trainable params: 1219418 (4.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbc09082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "WARNING:tensorflow:From c:\\Users\\user\\miniconda3\\envs\\venv1\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\user\\miniconda3\\envs\\venv1\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "644/644 [==============================] - 38s 49ms/step - loss: 6.9347 - accuracy: 0.0326 - val_loss: 6.6978 - val_accuracy: 0.0336\n",
      "Epoch 2/45\n",
      "644/644 [==============================] - 25s 38ms/step - loss: 6.4913 - accuracy: 0.0379 - val_loss: 6.7719 - val_accuracy: 0.0412\n",
      "Epoch 3/45\n",
      "644/644 [==============================] - 24s 38ms/step - loss: 6.3516 - accuracy: 0.0448 - val_loss: 6.8080 - val_accuracy: 0.0474\n",
      "Epoch 4/45\n",
      "644/644 [==============================] - 26s 40ms/step - loss: 6.2110 - accuracy: 0.0510 - val_loss: 6.8502 - val_accuracy: 0.0468\n",
      "Epoch 5/45\n",
      "644/644 [==============================] - 37s 58ms/step - loss: 6.0656 - accuracy: 0.0539 - val_loss: 6.8580 - val_accuracy: 0.0561\n",
      "Epoch 6/45\n",
      "644/644 [==============================] - 25s 39ms/step - loss: 5.9143 - accuracy: 0.0649 - val_loss: 6.8754 - val_accuracy: 0.0664\n",
      "Epoch 7/45\n",
      "644/644 [==============================] - 25s 38ms/step - loss: 5.7605 - accuracy: 0.0753 - val_loss: 6.9570 - val_accuracy: 0.0668\n",
      "Epoch 8/45\n",
      "644/644 [==============================] - 23s 36ms/step - loss: 5.6253 - accuracy: 0.0821 - val_loss: 7.0320 - val_accuracy: 0.0674\n",
      "Epoch 9/45\n",
      "644/644 [==============================] - 25s 38ms/step - loss: 5.4972 - accuracy: 0.0901 - val_loss: 7.0996 - val_accuracy: 0.0686\n",
      "Epoch 10/45\n",
      "644/644 [==============================] - 24s 37ms/step - loss: 5.3750 - accuracy: 0.0955 - val_loss: 7.1439 - val_accuracy: 0.0680\n",
      "Epoch 11/45\n",
      "644/644 [==============================] - 24s 37ms/step - loss: 5.2548 - accuracy: 0.1023 - val_loss: 7.2317 - val_accuracy: 0.0696\n",
      "Epoch 12/45\n",
      "644/644 [==============================] - 39s 60ms/step - loss: 5.1419 - accuracy: 0.1063 - val_loss: 7.3587 - val_accuracy: 0.0668\n",
      "Epoch 13/45\n",
      "644/644 [==============================] - 31s 49ms/step - loss: 5.0259 - accuracy: 0.1128 - val_loss: 7.3930 - val_accuracy: 0.0668\n",
      "Epoch 14/45\n",
      "644/644 [==============================] - 25s 39ms/step - loss: 4.9138 - accuracy: 0.1174 - val_loss: 7.5262 - val_accuracy: 0.0659\n",
      "Epoch 15/45\n",
      "644/644 [==============================] - 26s 40ms/step - loss: 4.7995 - accuracy: 0.1236 - val_loss: 7.6471 - val_accuracy: 0.0686\n",
      "Epoch 16/45\n",
      "644/644 [==============================] - 24s 37ms/step - loss: 4.6873 - accuracy: 0.1287 - val_loss: 7.7776 - val_accuracy: 0.0699\n",
      "Epoch 17/45\n",
      "644/644 [==============================] - 23s 36ms/step - loss: 4.5731 - accuracy: 0.1328 - val_loss: 7.9098 - val_accuracy: 0.0666\n",
      "Epoch 18/45\n",
      "644/644 [==============================] - 23s 36ms/step - loss: 4.4607 - accuracy: 0.1405 - val_loss: 8.0501 - val_accuracy: 0.0682\n",
      "Epoch 19/45\n",
      "644/644 [==============================] - 23s 35ms/step - loss: 4.3515 - accuracy: 0.1486 - val_loss: 8.1922 - val_accuracy: 0.0688\n",
      "Epoch 20/45\n",
      "644/644 [==============================] - 26s 41ms/step - loss: 4.2494 - accuracy: 0.1541 - val_loss: 8.3224 - val_accuracy: 0.0692\n",
      "Epoch 21/45\n",
      "644/644 [==============================] - 30s 47ms/step - loss: 4.1490 - accuracy: 0.1657 - val_loss: 8.4694 - val_accuracy: 0.0659\n",
      "Epoch 22/45\n",
      "644/644 [==============================] - 34s 52ms/step - loss: 4.0529 - accuracy: 0.1753 - val_loss: 8.6047 - val_accuracy: 0.0659\n",
      "Epoch 23/45\n",
      "644/644 [==============================] - 28s 44ms/step - loss: 3.9638 - accuracy: 0.1890 - val_loss: 8.7512 - val_accuracy: 0.0633\n",
      "Epoch 24/45\n",
      "644/644 [==============================] - 47s 73ms/step - loss: 3.8840 - accuracy: 0.2056 - val_loss: 8.8748 - val_accuracy: 0.0631\n",
      "Epoch 25/45\n",
      "644/644 [==============================] - 32s 50ms/step - loss: 3.8000 - accuracy: 0.2173 - val_loss: 9.0350 - val_accuracy: 0.0629\n",
      "Epoch 26/45\n",
      "644/644 [==============================] - 40s 62ms/step - loss: 3.7183 - accuracy: 0.2309 - val_loss: 9.1320 - val_accuracy: 0.0624\n",
      "Epoch 27/45\n",
      "644/644 [==============================] - 40s 62ms/step - loss: 3.6433 - accuracy: 0.2429 - val_loss: 9.2449 - val_accuracy: 0.0639\n",
      "Epoch 28/45\n",
      "644/644 [==============================] - 40s 62ms/step - loss: 3.5755 - accuracy: 0.2570 - val_loss: 9.3777 - val_accuracy: 0.0604\n",
      "Epoch 29/45\n",
      "644/644 [==============================] - 40s 62ms/step - loss: 3.5083 - accuracy: 0.2659 - val_loss: 9.5353 - val_accuracy: 0.0583\n",
      "Epoch 30/45\n",
      "644/644 [==============================] - 40s 62ms/step - loss: 3.4460 - accuracy: 0.2795 - val_loss: 9.6095 - val_accuracy: 0.0610\n",
      "Epoch 31/45\n",
      "644/644 [==============================] - 40s 63ms/step - loss: 3.3905 - accuracy: 0.2864 - val_loss: 9.7337 - val_accuracy: 0.0614\n",
      "Epoch 32/45\n",
      "644/644 [==============================] - 41s 63ms/step - loss: 3.3248 - accuracy: 0.3002 - val_loss: 9.8412 - val_accuracy: 0.0595\n",
      "Epoch 33/45\n",
      "644/644 [==============================] - 41s 63ms/step - loss: 3.2751 - accuracy: 0.3044 - val_loss: 9.9390 - val_accuracy: 0.0589\n",
      "Epoch 34/45\n",
      "644/644 [==============================] - 40s 62ms/step - loss: 3.2238 - accuracy: 0.3159 - val_loss: 10.0023 - val_accuracy: 0.0604\n",
      "Epoch 35/45\n",
      "644/644 [==============================] - 41s 64ms/step - loss: 3.1739 - accuracy: 0.3212 - val_loss: 10.0858 - val_accuracy: 0.0540\n",
      "Epoch 36/45\n",
      "644/644 [==============================] - 41s 63ms/step - loss: 3.1210 - accuracy: 0.3352 - val_loss: 10.1843 - val_accuracy: 0.0565\n",
      "Epoch 37/45\n",
      "644/644 [==============================] - 41s 63ms/step - loss: 3.0710 - accuracy: 0.3423 - val_loss: 10.2960 - val_accuracy: 0.0550\n",
      "Epoch 38/45\n",
      "644/644 [==============================] - 41s 64ms/step - loss: 3.0258 - accuracy: 0.3503 - val_loss: 10.3671 - val_accuracy: 0.0569\n",
      "Epoch 39/45\n",
      "644/644 [==============================] - 41s 64ms/step - loss: 2.9862 - accuracy: 0.3597 - val_loss: 10.4612 - val_accuracy: 0.0571\n",
      "Epoch 40/45\n",
      "644/644 [==============================] - 41s 64ms/step - loss: 2.9400 - accuracy: 0.3679 - val_loss: 10.5353 - val_accuracy: 0.0558\n",
      "Epoch 41/45\n",
      "644/644 [==============================] - 41s 64ms/step - loss: 2.8941 - accuracy: 0.3745 - val_loss: 10.6476 - val_accuracy: 0.0561\n",
      "Epoch 42/45\n",
      "644/644 [==============================] - 41s 64ms/step - loss: 2.8523 - accuracy: 0.3840 - val_loss: 10.7209 - val_accuracy: 0.0554\n",
      "Epoch 43/45\n",
      "644/644 [==============================] - 42s 64ms/step - loss: 2.8177 - accuracy: 0.3900 - val_loss: 10.7976 - val_accuracy: 0.0540\n",
      "Epoch 44/45\n",
      "644/644 [==============================] - 39s 61ms/step - loss: 2.7797 - accuracy: 0.3954 - val_loss: 10.8457 - val_accuracy: 0.0556\n",
      "Epoch 45/45\n",
      "644/644 [==============================] - 23s 35ms/step - loss: 2.7413 - accuracy: 0.4017 - val_loss: 10.9142 - val_accuracy: 0.0538\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(\n",
    "    Xtrain,ytrain,\n",
    "    validation_data = (Xtest,ytest),\n",
    "    epochs = 45,\n",
    "    verbose = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a719d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(mdoel,tokenizer,text,max_sequence_length):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "   \n",
    "    if len(token_list) >= max_sequence_length:\n",
    "        token_list = token_list[-(max_sequence_length-1):]\n",
    "    \n",
    "    token_list = pad_sequences([token_list],maxlen=max_sequence_length-1,padding='pre')\n",
    "\n",
    "    predicted = model.predict(token_list,verbose=1)\n",
    "\n",
    "    predicted_next_word = np.argmax(predicted,axis=1)\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index == predicted_next_word:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0db34fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text :To be or not to be\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted next Word is : buried\n"
     ]
    }
   ],
   "source": [
    "input_text = 'To be or not to be'\n",
    "print(f'Input text :{input_text}')\n",
    "max_sequence_length = model.input_shape[1]+1\n",
    "next_word = predict_next_word(model,tokenizer,input_text,max_sequence_length)\n",
    "print(f'Predicted next Word is : {next_word}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b8bc6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\venv1\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nUSE OF PROTOCOL\\n\\nEfficiency: Newer pickle protocols often introduce more efficient ways of serializing data, resulting in smaller file sizes and potentially faster loading/dumping times.\\n\\nFeatures: Newer protocols might support the serialization of a wider range of Python objects or handle certain data types more robustly.\\n\\nFuture-proofing (to some extent): By using the highest protocol, you're leveraging the latest advancements in the pickle module.\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('Next_word_LSTM.h5')\n",
    "\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle :\n",
    "    pickle.dump(tokenizer,handle,protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "'''\n",
    "USE OF PROTOCOL\n",
    "\n",
    "Efficiency: Newer pickle protocols often introduce more efficient ways of serializing data, resulting in smaller file sizes and potentially faster loading/dumping times.\n",
    "\n",
    "Features: Newer protocols might support the serialization of a wider range of Python objects or handle certain data types more robustly.\n",
    "\n",
    "Future-proofing (to some extent): By using the highest protocol, you're leveraging the latest advancements in the pickle module.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bed985da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text :That person is \n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted next Word is : cunning\n"
     ]
    }
   ],
   "source": [
    "input_text = 'That person is '\n",
    "print(f'Input text :{input_text}')\n",
    "max_sequence_length = model.input_shape[1]+1\n",
    "next_word = predict_next_word(model,tokenizer,input_text,max_sequence_length)\n",
    "print(f'Predicted next Word is : {next_word}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb03964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the GRU model\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(total_words,100,input_length = max_sequence_length-1))\n",
    "model2.add(GRU(150,return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(GRU(100))\n",
    "model2.add(Dense(total_words,activation = 'softmax'))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
